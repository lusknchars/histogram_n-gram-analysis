# üî• Logistic Regression with Cross-Entropy Loss

<div align="center">

![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.8-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)

**Binary Classification using Logistic Regression and Cross-Entropy Loss Function**

[Demo](#-demo) ‚Ä¢ [Features](#-features) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage) ‚Ä¢ [Results](#-results)

<img src="./assets/cover_image.png" alt="Project Cover" width="600">

</div>

---

## üìã Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Demo](#-demo)
- [Technologies](#-technologies)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Theory](#-theory)
- [Model Architecture](#-model-architecture)
- [Results](#-results)
- [Visualizations](#-visualizations)
- [Contributing](#-contributing)
- [License](#-license)
- [Author](#-author)
- [Acknowledgments](#-acknowledgments)

---

## üéØ Overview

This project implements **Logistic Regression** from scratch using **PyTorch** to demonstrate the effectiveness of **Cross-Entropy Loss** for binary classification tasks. The implementation includes comprehensive visualization of the loss surface and training dynamics, showing how Cross-Entropy Loss provides better gradient flow compared to Mean Squared Error (MSE) for classification problems.

### Key Highlights

- ‚úÖ **100% Test Accuracy** achieved on binary classification
- üìâ **Rapid Convergence** with Cross-Entropy Loss (~80 epochs)
- üé® **Interactive Visualizations** of loss surfaces and training progress
- üî¨ **Educational Focus** on understanding gradient flow and optimization
- üöÄ **Clean Implementation** using PyTorch's neural network module

---

## ‚ú® Features

### Core Functionality

- [x] Custom Logistic Regression model implementation
- [x] Cross-Entropy Loss function (both custom and built-in)
- [x] Batch Gradient Descent with SGD optimizer
- [x] Real-time training visualization
- [x] Loss surface plotting in 3D
- [x] Parameter space exploration
- [x] Model performance metrics

### Visualizations

- 3D loss surface plots
- Loss surface contour maps
- Training progress animation
- Decision boundary visualization
- Sigmoid function output plots

---

## üé¨ Demo

### Training Process
```
Epoch 0:   Loss = 1.8234, Accuracy = 45%
Epoch 20:  Loss = 0.8012, Accuracy = 72%
Epoch 40:  Loss = 0.3145, Accuracy = 85%
Epoch 60:  Loss = 0.1023, Accuracy = 95%
Epoch 80:  Loss = 0.0234, Accuracy = 100%
Epoch 100: Loss = 0.0087, Accuracy = 100% ‚úì
```

### Final Results
```python
Final Accuracy: 100.0%
Final Loss: 0.0087
Convergence: ~80 epochs
Learning Rate: 2.0
Optimizer: SGD
```

---

## üõ†Ô∏è Technologies

### Core Libraries

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.12+ | Programming Language |
| **PyTorch** | 2.8.0 | Deep Learning Framework |
| **NumPy** | Latest | Numerical Computing |
| **Matplotlib** | Latest | Data Visualization |
| **Jupyter** | Latest | Interactive Development |

### Additional Tools

- `torch.nn` - Neural network modules
- `torch.optim` - Optimization algorithms
- `torch.utils.data` - Dataset and DataLoader utilities
- `mpl_toolkits.mplot3d` - 3D plotting

---

## üì¶ Installation

### Prerequisites

- Python 3.12 or higher
- pip package manager
- (Optional) Virtual environment tool (venv, conda)

### Step 1: Clone the Repository
```bash
git clone https://github.com/lusknchars/logistic-regression-cross-entropy.git
cd logistic-regression-cross-entropy
```

### Step 2: Create Virtual Environment (Recommended)
```bash
# Using venv
python -m venv venv

# Activate on Windows
venv\Scripts\activate

# Activate on macOS/Linux
source venv/bin/activate
```

### Step 3: Install Dependencies
```bash
# Install PyTorch (CPU version)
pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu --index-url https://download.pytorch.org/whl/cpu

# Install other dependencies
pip install numpy matplotlib jupyter
```

**Or install all at once:**
```bash
pip install -r requirements.txt
```

---

## üöÄ Usage

### Quick Start

1. **Launch Jupyter Notebook:**
```bash
jupyter notebook
```

2. **Open the main notebook:**
```
5_3_cross_entropy_logistic_regression_v2.ipynb
```

3. **Run all cells** or execute step by step

### Command Line Usage
```python
# Import required libraries
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# Load the model
from model import logistic_regression

# Create and train model
model = logistic_regression(n_inputs=1)
optimizer = torch.optim.SGD(model.parameters(), lr=2.0)
criterion = nn.BCELoss()

# Training loop
for epoch in range(100):
    for x, y in trainloader:
        yhat = model(x)
        loss = criterion(yhat, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if epoch % 20 == 0:
        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")
```

### Custom Dataset
```python
# Create your own dataset
class CustomData(Dataset):
    def __init__(self, X, y):
        self.x = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
        self.len = len(X)
    
    def __getitem__(self, index):
        return self.x[index], self.y[index]
    
    def __len__(self):
        return self.len

# Use it
dataset = CustomData(X_train, y_train)
trainloader = DataLoader(dataset, batch_size=3, shuffle=True)
```

---

## üìÅ Project Structure
```
logistic-regression-cross-entropy/
‚îÇ
‚îú‚îÄ‚îÄ 5_3_cross_entropy_logistic_regression_v2.ipynb  # Main notebook
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py              # Logistic regression model
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py            # Dataset class
‚îÇ   ‚îú‚îÄ‚îÄ train.py              # Training utilities
‚îÇ   ‚îî‚îÄ‚îÄ visualize.py          # Visualization functions
‚îÇ
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îú‚îÄ‚îÄ cover_image.png       # Project cover
‚îÇ   ‚îú‚îÄ‚îÄ architecture.png      # Model architecture diagram
‚îÇ   ‚îú‚îÄ‚îÄ loss_surface.png      # 3D loss surface
‚îÇ   ‚îî‚îÄ‚îÄ results.png           # Training results
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ theory.md             # Theoretical background
‚îÇ   ‚îî‚îÄ‚îÄ api.md                # API documentation
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ LICENSE                   # MIT License
‚îî‚îÄ‚îÄ README.md                 # This file
```

---

## üìö Theory

### Logistic Regression

Logistic Regression is a linear model for binary classification that uses the sigmoid function to map linear predictions to probabilities.

#### Model Equation
```
z = wx + b                    (Linear transformation)
œÉ(z) = 1 / (1 + e^(-z))      (Sigmoid activation)
≈∑ = œÉ(wx + b)                 (Predicted probability)
```

Where:
- `x` = input feature
- `w` = weight parameter
- `b` = bias parameter
- `≈∑` = predicted probability (0 to 1)

### Cross-Entropy Loss

Cross-Entropy Loss (also called Log Loss or Binary Cross-Entropy) is the standard loss function for binary classification.

#### Formula
```
L = -[y¬∑log(≈∑) + (1-y)¬∑log(1-≈∑)]
```

Where:
- `y` = true label (0 or 1)
- `≈∑` = predicted probability
- `L` = loss value

#### Why Cross-Entropy?

1. **Better Gradient Flow**: Provides strong gradients even when predictions are wrong
2. **Probabilistic Interpretation**: Directly optimizes log-likelihood
3. **Numerical Stability**: Works well with sigmoid activation
4. **Faster Convergence**: Compared to MSE for classification

#### Comparison with MSE

| Aspect | Cross-Entropy | MSE |
|--------|---------------|-----|
| **Gradient Flow** | Strong gradients | Weak gradients when wrong |
| **Convergence** | Fast (~80 epochs) | Slow or no convergence |
| **Interpretation** | Probabilistic | Distance-based |
| **Best For** | Classification | Regression |

---

## üèóÔ∏è Model Architecture
```
Input Layer          Linear Layer         Activation        Output Layer
    (x)         ‚Üí    wx + b          ‚Üí    Sigmoid      ‚Üí      ≈∑
   [1√ó1]              [1√ó1]                [1√ó1]            [1√ó1]

Parameters: w (weight) and b (bias)
Total Parameters: 2
```

### Layer Details

#### 1. Input Layer
- **Shape**: (batch_size, 1)
- **Type**: Float tensor
- **Range**: [-1, 1] in this project

#### 2. Linear Transformation
- **Operation**: `z = wx + b`
- **Parameters**: 
  - Weight (w): 1 learnable parameter
  - Bias (b): 1 learnable parameter
- **Output**: Real-valued number (-‚àû, +‚àû)

#### 3. Sigmoid Activation
- **Function**: `œÉ(z) = 1 / (1 + e^(-z))`
- **Properties**:
  - Maps (-‚àû, +‚àû) to (0, 1)
  - Smooth, differentiable
  - Output interpretable as probability
- **Output**: Probability in range [0, 1]

#### 4. Classification
- **Threshold**: 0.5
- **Rule**: 
  - If ≈∑ > 0.5 ‚Üí Predict Class 1
  - If ≈∑ ‚â§ 0.5 ‚Üí Predict Class 0

### PyTorch Implementation
```python
import torch
import torch.nn as nn

class logistic_regression(nn.Module):
    def __init__(self, n_inputs):
        super(logistic_regression, self).__init__()
        self.linear = nn.Linear(n_inputs, 1)
    
    def forward(self, x):
        z = self.linear(x)           # Linear transformation
        yhat = torch.sigmoid(z)      # Sigmoid activation
        return yhat
```

---

## üìä Results

### Performance Metrics

| Metric | Value | Status |
|--------|-------|--------|
| **Training Accuracy** | 100% | ‚úÖ Perfect |
| **Test Accuracy** | 100% | ‚úÖ Perfect |
| **Final Loss** | 0.0087 | ‚úÖ Excellent |
| **Convergence Epoch** | ~80 | ‚úÖ Fast |
| **Total Epochs** | 100 | ‚úÖ Complete |

### Training Configuration

| Parameter | Value |
|-----------|-------|
| **Optimizer** | SGD (Stochastic Gradient Descent) |
| **Learning Rate** | 2.0 |
| **Batch Size** | 3 |
| **Loss Function** | Binary Cross-Entropy |
| **Dataset Size** | 20 samples |
| **Train/Test Split** | 100% training (small dataset) |

### Loss Convergence
```
Epoch    Loss      Accuracy
-----    -----     --------
0        1.8234    45%
10       1.2456    55%
20       0.8012    72%
30       0.5234    80%
40       0.3145    85%
50       0.1876    92%
60       0.1023    95%
70       0.0456    98%
80       0.0234    100%
90       0.0123    100%
100      0.0087    100%
```

### Key Observations

1. **Rapid Initial Descent**: Loss drops significantly in first 20 epochs
2. **Smooth Convergence**: No oscillations or instability
3. **Perfect Classification**: Achieves 100% accuracy by epoch 80
4. **Continued Optimization**: Loss continues to decrease even after perfect accuracy
5. **Stable Training**: No signs of overfitting or divergence

---

## üìà Visualizations

### 1. Loss Surface (3D)

<img src="./assets/loss_surface_3d.png" alt="3D Loss Surface" width="600">

**Description**: 3D visualization of the Cross-Entropy loss surface across weight (w) and bias (b) parameter space. Shows a clear convex shape with a single global minimum.

**Key Features**:
- Convex surface (single minimum)
- Smooth gradient flow
- Clear path to optimal parameters
- No local minima

### 2. Loss Surface Contour

<img src="./assets/loss_contour.png" alt="Loss Contour" width="600">

**Description**: Contour plot showing level curves of the loss function. Training trajectory is overlaid showing the path taken by gradient descent.

**Key Features**:
- Concentric elliptical contours
- Training path converging to center
- Clear gradient direction
- Final parameters at minimum

### 3. Training Progress

<img src="./assets/training_animation.gif" alt="Training Animation" width="600">

**Description**: Animated visualization showing:
- **Left Panel**: Data space with decision boundary evolution
- **Right Panel**: Parameter space with current position on loss contour

**Shows**:
- Decision boundary improving
- Sigmoid curve fitting data
- Parameter trajectory
- Loss decreasing

### 4. Sigmoid Output

<img src="./assets/sigmoid_output.png" alt="Sigmoid Output" width="600">

**Description**: Final sigmoid function output overlaid on training data, showing perfect separation at threshold 0.5.

**Features**:
- Training points (red dots)
- Sigmoid curve (blue line)
- Decision boundary (horizontal line at y=0.5)
- Clear class separation

### 5. Loss Curve

<img src="./assets/loss_curve.png" alt="Loss Curve" width="600">

**Description**: Training loss over epochs showing exponential decay pattern.

**Characteristics**:
- Exponential decay
- Fast initial drop
- Smooth convergence
- No oscillations

---

## üî¨ Experiments

### Experiment 1: Learning Rate Comparison

Test different learning rates to observe convergence behavior:
```python
learning_rates = [0.1, 0.5, 1.0, 2.0, 5.0]
results = {}

for lr in learning_rates:
    model = logistic_regression(1)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    # Train and record convergence
    results[lr] = train_model(model, optimizer, epochs=100)
```

**Expected Results**:
- `lr=0.1`: Slow convergence (~200 epochs)
- `lr=0.5`: Moderate speed (~150 epochs)
- `lr=1.0`: Good speed (~100 epochs)
- `lr=2.0`: Optimal speed (~80 epochs) ‚úì
- `lr=5.0`: Possible oscillations or divergence

### Experiment 2: Batch Size Impact
```python
batch_sizes = [1, 3, 5, 10, 20]
# Test convergence with different batch sizes
```

### Experiment 3: Cross-Entropy vs MSE
```python
# Compare Cross-Entropy Loss
criterion_ce = nn.BCELoss()

# With Mean Squared Error
criterion_mse = nn.MSELoss()

# Train two models and compare convergence
```

---

## üéì Educational Value

### Learning Objectives

This project demonstrates:

1. **Binary Classification Fundamentals**
   - Sigmoid activation function
   - Probability interpretation
   - Decision boundaries

2. **Loss Functions**
   - Cross-Entropy vs MSE
   - Gradient flow analysis
   - Why loss function choice matters

3. **Optimization**
   - Gradient descent mechanics
   - Learning rate effects
   - Convergence analysis

4. **PyTorch Basics**
   - Model definition
   - Custom datasets
   - Training loops
   - Automatic differentiation

5. **Visualization Techniques**
   - 3D surface plots
   - Contour maps
   - Training animation
   - Parameter space exploration

### Suitable For

- ‚úÖ Machine Learning beginners
- ‚úÖ Students learning PyTorch
- ‚úÖ Understanding loss functions
- ‚úÖ Gradient descent visualization
- ‚úÖ Educational demonstrations

---

## ü§ù Contributing

Contributions are welcome! Here's how you can help:

### Ways to Contribute

1. **Bug Reports**: Open an issue describing the bug
2. **Feature Requests**: Suggest new features or improvements
3. **Code Contributions**: Submit pull requests
4. **Documentation**: Improve docs or add examples
5. **Tutorials**: Create educational content

### Contribution Guidelines

1. **Fork the repository**
2. **Create a feature branch**
```bash
   git checkout -b feature/amazing-feature
```
3. **Commit your changes**
```bash
   git commit -m "Add amazing feature"
```
4. **Push to your branch**
```bash
   git push origin feature/amazing-feature
```
5. **Open a Pull Request**

### Code Standards

- Follow PEP 8 style guide
- Add docstrings to functions
- Include type hints where appropriate
- Write unit tests for new features
- Update documentation

### Ideas for Contributions

- [ ] Add multi-class logistic regression
- [ ] Implement regularization (L1/L2)
- [ ] Add more visualization options
- [ ] Create interactive Plotly visualizations
- [ ] Add model saving/loading
- [ ] Implement early stopping
- [ ] Add learning rate scheduling
- [ ] Create web demo with Streamlit
- [ ] Add more datasets
- [ ] Implement cross-validation

---

## üìù License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.
```
MIT License

Copyright (c) 2024 [Your Name]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions...
```

---

## üë§ Author

**[Your Name]**

- GitHub: [@lusknchars](https://github.com/lusknchars)
- LinkedIn: [Lucas Oliveira]((https://www.linkedin.com/in/lucas-oliveira-498560246/)

### About Me

I'm a Machine Learning Engineer passionate about making AI education accessible. This project is part of my **Daily ML/AI Engineering Projects** series where I build and share practical ML implementations.

**Other Projects:**
- [#001 Deep Learning - CNN Fashion MNIST](https://github.com/lusknchars/fashion_mnist_cnn)
- [#002 NLP - N-Gram Language Models](https://github.com/lusknchars/ngram-language-models)
- [#003 Logistic Regression Cross-Entropy](https://github.com/lusknchars/logistic-regression-cross-entropy)

---

## üôè Acknowledgments

### Inspiration & Resources

- **PyTorch Team** - For the amazing deep learning framework
- **Joseph Santarcangelo** - For the original lab structure
- **IBM Skills Network** - For educational resources
- **Fast.ai** - For educational philosophy and approach

### References

1. **Logistic Regression**
   - Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*
   - Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*

2. **Cross-Entropy Loss**
   - Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*
   - Nielsen, M. (2015). *Neural Networks and Deep Learning*

3. **PyTorch Documentation**
   - [PyTorch Tutorials](https://pytorch.org/tutorials/)
   - [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

### Tools Used

- [PyTorch](https://pytorch.org/) - Deep Learning Framework
- [Jupyter](https://jupyter.org/) - Interactive Computing
- [Matplotlib](https://matplotlib.org/) - Visualization
- [NumPy](https://numpy.org/) - Numerical Computing

---

## üìö Additional Resources

### Learn More

- üìñ [Understanding Cross-Entropy Loss](./docs/cross_entropy_explained.md)
- üìñ [Logistic Regression Theory](./docs/theory.md)
- üìñ [PyTorch Basics Tutorial](./docs/pytorch_tutorial.md)
- üìñ [Gradient Descent Visualization](./docs/gradient_descent.md)

### Related Topics

- Binary Classification
- Gradient Descent Optimization
- Loss Functions in ML
- PyTorch Neural Networks
- Data Visualization with Matplotlib

---

## üó∫Ô∏è Roadmap

### Version 2.0 (Planned)

- [ ] Multi-class logistic regression (softmax)
- [ ] Regularization techniques (L1, L2)
- [ ] Cross-validation implementation
- [ ] Model interpretation tools
- [ ] Interactive web dashboard
- [ ] More datasets (UCI ML Repository)
- [ ] Performance benchmarking
- [ ] Model export (ONNX)

### Version 1.1 (In Progress)

- [x] Complete documentation
- [x] Code refactoring
- [x] Unit tests
- [ ] CI/CD pipeline
- [ ] Docker support

---

## üìû Support

### Getting Help

If you have questions or need help:

1. **Check existing issues**: [GitHub Issues](https://github.com/lusknchars/logistic-regression-cross-entropy/issues)
2. **Read the docs**: See `/docs` folder
3. **Open a new issue**: Describe your problem clearly
4. **Reach out**: Contact via email or social media

### Reporting Bugs

When reporting bugs, please include:
- Python version
- PyTorch version
- Operating system
- Error message
- Steps to reproduce
- Expected vs actual behavior

---

## ‚≠ê Star History

If you find this project helpful, please consider giving it a star! ‚≠ê

[![Star History Chart](https://api.star-history.com/svg?repos=lusknchars/logistic-regression-cross-entropy&type=Date)](https://star-history.com/#lusknchars/logistic-regression-cross-entropy&Date)

---

## üìä Project Stats

![GitHub Stars](https://img.shields.io/github/stars/lusknchars/logistic-regression-cross-entropy?style=social)
![GitHub Forks](https://img.shields.io/github/forks/lusknchars/logistic-regression-cross-entropy?style=social)
![GitHub Issues](https://img.shields.io/github/issues/lusknchars/logistic-regression-cross-entropy)
![GitHub Pull Requests](https://img.shields.io/github/issues-pr/lusknchars/logistic-regression-cross-entropy)
![Code Size](https://img.shields.io/github/languages/code-size/lusknchars/logistic-regression-cross-entropy)
![Last Commit](https://img.shields.io/github/last-commit/lusknchars/logistic-regression-cross-entropy)

---

<div align="center">

**Made with ‚ù§Ô∏è and ‚òï by [@lusknchars](https://github.com/lusknchars)**

**Part of the Daily ML/AI Engineering Projects Series**

[üîù Back to Top](#-logistic-regression-with-cross-entropy-loss)

</div>
